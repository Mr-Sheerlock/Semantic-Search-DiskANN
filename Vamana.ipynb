{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def L2_square_float(pa, pb, pd):\n",
    "    diff = np.array(pa) - np.array(pb)\n",
    "    return np.sum(diff ** 2)\n",
    "\n",
    "def L2_square_int(pa, pb, pd):\n",
    "    diff = np.array(pa) - np.array(pb)\n",
    "    return np.sum(diff ** 2)\n",
    "\n",
    "def inner_product_float(pa, pb, pd):\n",
    "    return np.dot(np.array(pa), np.array(pb))\n",
    "\n",
    "def inner_product_int(pa, pb, pd):\n",
    "    return np.dot(np.array(pa), np.array(pb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MetricType:\n",
    "    L2 = 100\n",
    "    IP = 200\n",
    "\n",
    "class DataType:\n",
    "    INT32 = 100\n",
    "    FLOAT = 200\n",
    "    UINT8 = 300\n",
    "\n",
    "class MetricSpace:\n",
    "    def __init__(self, dim):\n",
    "        self.dis_func_ = None\n",
    "        self.esize = None\n",
    "\n",
    "    def full_dist(self, pa, pb, pd):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def half_dist(self, pa, pb, pd):\n",
    "        dim = int(pd[0]) >> 1\n",
    "        return self.full_dist(pa, pb, np.array([dim]))\n",
    "\n",
    "    def post_half(self, pa, pb, pd):\n",
    "        dim = int(pd[0])\n",
    "        ppa = np.array(pa)\n",
    "        ppb = np.array(pb)\n",
    "        return self.full_dist(ppa[(dim >> 1) * self.esize:], ppb[(dim >> 1) * self.esize:], np.array([dim - (dim >> 1)]))\n",
    "\n",
    "class L2SpaceF(MetricSpace):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__(dim)\n",
    "        self.dis_func_ = self.L2_square_f\n",
    "        self.esize = np.dtype('float32').itemsize\n",
    "\n",
    "    def full_dist(self, pa, pb, pd):\n",
    "        return self.dis_func_(pa, pb, pd)\n",
    "\n",
    "    @staticmethod\n",
    "    def L2_square_f(pa, pb, pd):\n",
    "        diff = np.array(pa) - np.array(pb)\n",
    "        return np.sum(diff ** 2).item()\n",
    "\n",
    "class IPSapceF(MetricSpace):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__(dim)\n",
    "        self.dis_func_ = self.inner_product_f\n",
    "        self.esize = np.dtype('float32').itemsize\n",
    "\n",
    "    def full_dist(self, pa, pb, pd):\n",
    "        return self.dis_func_(pa, pb, pd)\n",
    "\n",
    "    @staticmethod\n",
    "    def inner_product_f(pa, pb, pd):\n",
    "        return np.dot(np.array(pa), np.array(pb)).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ctypes\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import heapq\n",
    "import threading\n",
    "import time\n",
    "import concurrent.futures\n",
    "\n",
    "class Vamana:\n",
    "    def __init__(self, mt, dt, r, l, alp, dim):\n",
    "        self.R_ = r\n",
    "        self.L_ = l\n",
    "        self.alpha_ = alp\n",
    "        self.dim_ = dim\n",
    "        self.link_list_locks_ = [0] * 1000000\n",
    "\n",
    "        if mt == \"L2\":\n",
    "            # todo: init by DataType, specified float temporarily\n",
    "            self.ms_ = L2SpaceF(dim)\n",
    "        elif mt == \"IP\":\n",
    "            self.ms_ = IPSapceF(dim)\n",
    "\n",
    "        # todo: default deal with float data\n",
    "        self.data_size_ = dim * 4  # assuming 4 bytes per float\n",
    "        self.link_size_ = self.R_ * 4 + 4  # assuming 4 bytes per idx_t\n",
    "        self.node_size_ = self.link_size_ + self.data_size_\n",
    "        self.index_built_ = False\n",
    "        self.sp_ = 0\n",
    "        self.graph_ = None\n",
    "        self.ntotal_ = 0\n",
    "\n",
    "    def __del__(self):\n",
    "        self.drop_index()\n",
    "\n",
    "    def drop_index(self):\n",
    "        if self.graph_:\n",
    "            del self.graph_\n",
    "        self.graph_ = None\n",
    "        self.index_built_ = False\n",
    "\n",
    "    def create_index(self, pdata, n):\n",
    "        if self.index_built_:\n",
    "            print(\"FBI Warning: index already built, if want re-build, call DropIndex first\")\n",
    "            return\n",
    "\n",
    "        self.ntotal_ = n\n",
    "        self.graph_ = ctypes.create_string_buffer(self.node_size_ * n)\n",
    "        ctypes.memset(self.graph_, 0, self.node_size_ * self.ntotal_)\n",
    "\n",
    "        if self.R_ < math.ceil(math.log2(self.ntotal_)):\n",
    "            print(\"FBI Warning: the parameter is less than log2(n), maybe result in low recall\")\n",
    "\n",
    "        self.link_list_locks_ = [threading.Lock() for _ in range(self.ntotal_)]\n",
    "        self.add_points(pdata, self.ntotal_)\n",
    "        self.random_init()\n",
    "        self.healthy_check()\n",
    "        self.build_index(pdata)\n",
    "        self.index_built_ = True\n",
    "\n",
    "    # Search from different starting points\n",
    "    def search(self, pquery, topk):\n",
    "        ret = [(0, 0) for _ in range(topk)]\n",
    "        ans = self.search(pquery, self.sp_, topk)\n",
    "        sz = len(ans)\n",
    "        while ans:\n",
    "            sz -= 1\n",
    "            ret[sz] = ans[0]  # Assuming ans is a list of tuples\n",
    "            heapq.heappop(ans)\n",
    "        return ret\n",
    "\n",
    "    def search_multiple_queries(self, pqueries, topk):\n",
    "        ret = []\n",
    "        # Search parallel for each query\n",
    "        for i in range(len(pqueries)):\n",
    "            ret.append(self.search_parallel(pqueries[i], topk))\n",
    "        return ret\n",
    "\n",
    "    def search_parallel(self, pqueries, topk):\n",
    "        with Pool() as pool:\n",
    "            results = pool.starmap(self.search, [(query, topk) for query in pqueries])\n",
    "        return results\n",
    "    \n",
    "    def healthy_check(self):\n",
    "        degree_hist = self.scan_graph()\n",
    "        print(\"show degree histogram of graph:\")\n",
    "        for i, cnt in enumerate(degree_hist):\n",
    "            print(f\"degree = {i}: cnt = {cnt}\")\n",
    "\n",
    "\n",
    "    def get_link_by_id(self, idx):\n",
    "        return ctypes.cast(ctypes.addressof(self.graph_) + self.node_size_ * idx, ctypes.POINTER(ctypes.c_int)).contents\n",
    "\n",
    "\n",
    "    def get_data_by_id(self, idx):\n",
    "        return ctypes.cast(ctypes.addressof(self.graph_) + self.node_size_ * idx + self.link_size_, ctypes.POINTER(ctypes.c_char)).contents\n",
    "\n",
    "\n",
    "    def add_points(self, pdata, n):\n",
    "        pd = ctypes.cast(pdata, ctypes.POINTER(ctypes.c_char))\n",
    "        for i in range(n):\n",
    "            ctypes.memmove(ctypes.addressof(self.graph_) + i * self.node_size_ + self.link_size_, pd + i * self.data_size_, self.data_size_)\n",
    "\n",
    "\n",
    "    def random_init(self):\n",
    "        random.seed()  # Seed the random number generator\n",
    "        num_threads = threading.active_count()  # Number of threads\n",
    "\n",
    "        # Using a lock for thread safety\n",
    "        lock = threading.Lock()\n",
    "\n",
    "        def worker(start, end):\n",
    "            for i in range(start, end):\n",
    "                if i % num_threads == threading.current_thread().ident % num_threads:\n",
    "                    random_neighbors = set()\n",
    "                    while len(random_neighbors) < self.R_:\n",
    "                        random_neighbors.add(random.randint(0, self.ntotal_ - 1))\n",
    "\n",
    "                    with lock:\n",
    "                        p_link = self.get_link_by_id(i)\n",
    "                        assert len(random_neighbors) <= self.R_\n",
    "                        for chosen in random_neighbors:\n",
    "                            p_link.contents.value += 1\n",
    "                            p_link.contents[p_link.contents.value] = chosen\n",
    "\n",
    "        # Split the work among threads\n",
    "        threads = []\n",
    "        chunk_size = self.ntotal_ // num_threads\n",
    "        for i in range(num_threads):\n",
    "            start = i * chunk_size\n",
    "            end = (i + 1) * chunk_size if i < num_threads - 1 else self.ntotal_\n",
    "            thread = threading.Thread(target=worker, args=(start, end))\n",
    "            thread.start()\n",
    "            threads.append(thread)\n",
    "\n",
    "        # Wait for all threads to finish\n",
    "        for thread in threads:\n",
    "            thread.join()\n",
    "\n",
    "    \n",
    "    def build_index(self, pdata):\n",
    "        assert self.ntotal_ > 0\n",
    "        pd = np.frombuffer(pdata, dtype=np.float32)\n",
    "        center = np.zeros(self.dim_, dtype=np.float32)\n",
    "\n",
    "        # Step 1: Calculate start point, i.e., navigate point in NSG\n",
    "        for i in range(self.ntotal_ * self.dim_):\n",
    "            center[i % self.dim_] += pd[i]\n",
    "\n",
    "        center /= self.ntotal_\n",
    "\n",
    "        tstart = time.time()\n",
    "        tpL = self.search(center, np.random.randint(0, self.ntotal_), self.L_)\n",
    "        tend = time.time()\n",
    "        print(f\"first search 4 sp_ finished in {tend - tstart:.3f} seconds.\")\n",
    "\n",
    "        while tpL:\n",
    "            self.sp_ = tpL[0][1]\n",
    "            tpL.pop()\n",
    "\n",
    "        print(f\"init sp_ = {self.sp_}\")\n",
    "\n",
    "        # Step 2: Do the first iteration with alpha = 1\n",
    "        tstart = time.time()\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            futures = []\n",
    "            for i in range(self.ntotal_):\n",
    "                future = executor.submit(self.search, pd[i * self.dim_: (i + 1) * self.dim_], self.L_)\n",
    "                futures.append((i, future))\n",
    "\n",
    "            for i, future in futures:\n",
    "                candidates = future.result()\n",
    "                self.robust_prune(i, candidates, 1.0, 1)\n",
    "                self.make_edge(i, 1.0)\n",
    "\n",
    "        tend = time.time()\n",
    "        print(f\"the first round iteration finished in {tend - tstart:.3f} seconds.\")\n",
    "\n",
    "        # Todo: Need to update sp_?\n",
    "        tpL = self.search(center, self.sp_, self.L_)\n",
    "        while tpL:\n",
    "            self.sp_ = tpL[0][1]\n",
    "            tpL.pop()\n",
    "\n",
    "        print(f\"updated sp_ after 1st iteration: {self.sp_}\")\n",
    "\n",
    "        print(\"HealthyCheck after the 1st round iteration:\")\n",
    "        self.healthy_check()\n",
    "\n",
    "        # Step 3: Do the second iteration with alpha = alpha_\n",
    "        tstart = time.time()\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            futures = []\n",
    "            for i in range(self.ntotal_ - 1, -1, -1):\n",
    "                future = executor.submit(self.search, pd[i * self.dim_: (i + 1) * self.dim_], self.L_)\n",
    "                futures.append((i, future))\n",
    "\n",
    "            for i, future in futures:\n",
    "                candidates = future.result()\n",
    "                self.robust_prune(i, candidates, self.alpha_, 2)\n",
    "                self.make_edge(i, self.alpha_)\n",
    "\n",
    "        tend = time.time()\n",
    "        print(f\"the second round iteration finished in {tend - tstart:.3f} seconds.\")\n",
    "\n",
    "        # Step 4: Update sp_\n",
    "        tpL = self.search(center, self.sp_, self.L_)\n",
    "        while tpL:\n",
    "            self.sp_ = tpL[0][1]\n",
    "            tpL.pop()\n",
    "\n",
    "        print(f\"updated sp_ after 2nd iteration: {self.sp_}\")\n",
    "\n",
    "        print(\"HealthyCheck after the 2nd round iteration:\")\n",
    "        self.healthy_check()\n",
    "\n",
    "    def search(self, qp, neighbor_candi):\n",
    "        while neighbor_candi:\n",
    "            heapq.heappop(neighbor_candi)\n",
    "            print(\"neighbor_candi not empty\")\n",
    "\n",
    "        vis = [False] * self.ntotal_\n",
    "        resultset = []\n",
    "        expandset = []\n",
    "        heapq.heappush(expandset, (-self.ms_.full_dist(self.get_data_by_id(self.sp_), qp, self.dim_), self.sp_))\n",
    "        vis[self.sp_] = True\n",
    "        lower_bound = -expandset[0][0]\n",
    "        heapq.heappush(neighbor_candi, (expandset[0][0], expandset[0][1]))\n",
    "\n",
    "        while expandset:\n",
    "            cur = heapq.heappop(expandset)\n",
    "            assert cur[1] < self.ntotal_\n",
    "\n",
    "            if -cur[0] > lower_bound:\n",
    "                break\n",
    "\n",
    "            link = self.get_link_by_id(cur[1])\n",
    "            linksz = link.contents.value\n",
    "\n",
    "            if linksz > self.R_:\n",
    "                print(f\"search: linksz = {linksz} which is > R_ = {self.R_}\")\n",
    "\n",
    "            assert linksz <= self.R_\n",
    "\n",
    "            with threading.Lock():\n",
    "                for i in range(1, linksz + 1):\n",
    "                    candi_id = link[i]\n",
    "\n",
    "                    if vis[candi_id]:\n",
    "                        continue\n",
    "\n",
    "                    candi_data = self.get_data_by_id(candi_id)\n",
    "                    dist = self.ms_.full_dist(qp, candi_data, self.dim_)\n",
    "\n",
    "                    if len(resultset) < self.L_ or dist < lower_bound:\n",
    "                        heapq.heappush(expandset, (-dist, candi_id))\n",
    "                        vis[candi_id] = True\n",
    "                        heapq.heappush(neighbor_candi, (dist, candi_id))\n",
    "                        heapq.heappush(resultset, (dist, candi_id))\n",
    "\n",
    "                        if len(resultset) > self.L_:\n",
    "                            heapq.heappop(resultset)\n",
    "\n",
    "                        if resultset:\n",
    "                            lower_bound = resultset[0][0]\n",
    "\n",
    "        return resultset\n",
    "    \n",
    "    \n",
    "    def search(self, qp, sp, topk):\n",
    "        ub = max(self.L_, topk)\n",
    "        vis = [False] * self.ntotal_\n",
    "        resultset = []\n",
    "        expandset = []\n",
    "        heapq.heappush(expandset, (-self.ms_.full_dist(self.get_data_by_id(sp), qp, self.dim_), sp))\n",
    "        vis[sp] = True\n",
    "        lower_bound = -expandset[0][0]\n",
    "\n",
    "        while expandset:\n",
    "            cur = heapq.heappop(expandset)\n",
    "            assert cur[1] < self.ntotal_\n",
    "\n",
    "            if -cur[0] > lower_bound:\n",
    "                break\n",
    "\n",
    "            link = self.get_link_by_id(cur[1])\n",
    "            linksz = link.contents.value\n",
    "\n",
    "            if linksz > self.R_:\n",
    "                print(f\"search_st: linksz = {linksz} which is > R_ = {self.R_}\")\n",
    "\n",
    "            assert linksz <= self.R_\n",
    "\n",
    "            for i in range(1, linksz + 1):\n",
    "                candi_id = link[i]\n",
    "\n",
    "                if vis[candi_id]:\n",
    "                    continue\n",
    "\n",
    "                candi_data = self.get_data_by_id(candi_id)\n",
    "                dist = self.ms_.full_dist(qp, candi_data, self.dim_)\n",
    "\n",
    "                if len(resultset) < ub or dist < lower_bound:\n",
    "                    heapq.heappush(expandset, (-dist, candi_id))\n",
    "                    vis[candi_id] = True\n",
    "                    heapq.heappush(resultset, (dist, candi_id))\n",
    "\n",
    "                    if len(resultset) > ub:\n",
    "                        heapq.heappop(resultset)\n",
    "\n",
    "                    if resultset:\n",
    "                        lower_bound = resultset[0][0]\n",
    "\n",
    "        return resultset\n",
    "    \n",
    "\n",
    "    def robust_prune(self, p, cand_set, alpha, flag):\n",
    "        with threading.Lock():\n",
    "            link = self.get_link_by_id(p)\n",
    "\n",
    "            if len(cand_set) <= self.R_:\n",
    "                link.contents.value = len(cand_set)\n",
    "                for i in range(1, link.contents.value + 1):\n",
    "                    link[i] = cand_set[0][1]\n",
    "                    cand_set.pop()\n",
    "\n",
    "                return\n",
    "\n",
    "            link.contents.value = 0\n",
    "\n",
    "            while cand_set:\n",
    "                if link.contents.value >= self.R_:\n",
    "                    break\n",
    "\n",
    "                cur = cand_set[0]\n",
    "                cand_set.pop()\n",
    "\n",
    "                good = True\n",
    "\n",
    "                for j in range(1, link.contents.value + 1):\n",
    "                    dist = self.ms_.full_dist(self.get_data_by_id(cur[1]), self.get_data_by_id(link[j]), self.dim_)\n",
    "                    if dist * alpha < -cur[0]:\n",
    "                        good = False\n",
    "                        break\n",
    "\n",
    "                if good:\n",
    "                    link.contents.value += 1\n",
    "                    link[link.contents.value] = cur[1]\n",
    "\n",
    "\n",
    "    def is_duplicate(self, p, link):\n",
    "        assert link.contents.value <= self.R_\n",
    "        for i in range(1, link.contents.value + 1):\n",
    "            if p == link[i]:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "\n",
    "    def make_edge(self, p, alpha):\n",
    "        link = self.get_link_by_id(p)\n",
    "        for i in range(1, link.contents.value + 1):\n",
    "            neighbor_link = self.get_link_by_id(link[i])\n",
    "\n",
    "            with threading.Lock():\n",
    "                if not self.is_duplicate(p, neighbor_link):\n",
    "                    if neighbor_link.contents.value < self.R_:\n",
    "                        neighbor_link.contents.value += 1\n",
    "                        neighbor_link[neighbor_link.contents.value] = p\n",
    "                    else:\n",
    "                        prune_candi = []\n",
    "                        dist = self.ms_.full_dist(self.get_data_by_id(p), self.get_data_by_id(link[i]), self.dim_)\n",
    "                        heapq.heappush(prune_candi, (-dist, p))\n",
    "                        for j in range(1, neighbor_link.contents.value + 1):\n",
    "                            heapq.heappush(prune_candi, (-self.ms_.full_dist(self.get_data_by_id(link[i]), self.get_data_by_id(neighbor_link[j]), self.dim_), neighbor_link[j]))\n",
    "\n",
    "                        self.robust_prune(link[i], prune_candi, alpha, 3)\n",
    "    \n",
    "    \n",
    "    def scan_graph(self, degree_histogram):\n",
    "        degree_histogram = [0] * (self.R_ + 1)\n",
    "        total = 0\n",
    "\n",
    "        def process_node(i):\n",
    "            link = self.get_link_by_id(i)\n",
    "\n",
    "            with threading.Lock():\n",
    "                if link.contents.value > self.R_:\n",
    "                    print(f\"scan_graph: *(link) = {link.contents.value} which is > R_ = {self.R_}\")\n",
    "\n",
    "                assert link.contents.value <= self.R_\n",
    "                degree_histogram[link.contents.value] += 1\n",
    "\n",
    "            ns = set()\n",
    "\n",
    "            for j in range(1, link.contents.value + 1):\n",
    "                assert link[j] < self.ntotal_\n",
    "                ns.add(link[j])\n",
    "\n",
    "            nonlocal total\n",
    "            total += abs(link.contents.value - len(ns))\n",
    "\n",
    "        with threading.Lock():\n",
    "            for i in range(self.ntotal_):\n",
    "                process_node(i)\n",
    "\n",
    "        print(f\"scan_graph done, duplicate total = {total}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
